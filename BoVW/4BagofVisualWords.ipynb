{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Bag of Visual Words Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 3: Zuzanna Szafranowska, Claudia Baca, Quim Comas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "#inPython3.x:\n",
    "import pickle as cPickle\n",
    "#in Python2.x:\n",
    "#import cPickle\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_fscore_support\n",
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first read the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images_filenames = cPickle.load(open('train_images_filenames.dat','rb'))\n",
    "test_images_filenames = cPickle.load(open('test_images_filenames.dat','rb'))\n",
    "train_labels = cPickle.load(open('train_labels.dat','rb'))\n",
    "test_labels = cPickle.load(open('test_labels.dat','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature extraction, descriptors</h1>\n",
    "We create detectors and descriptors: SIFT, SURF, ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIFTdetector = cv2.xfeatures2d.SIFT_create(nfeatures=300)\n",
    "SURFdetector = cv2.xfeatures2d.SURF_create(300)\n",
    "ORBdetector = cv2.ORB_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the type of local feature detector that we want to use. \n",
    "\n",
    "- 1 ==> SIFT\n",
    "- 2 ==> SURF\n",
    "- 3 ==> ORB\n",
    "- 4 ==> Dense SIFT\n",
    "- 5 ==> Dense SURF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=4\n",
    "#x = input(\"Enter a number to choose the local feature detector: \")\n",
    "L=2\n",
    "#Enter the number levels of the spatial pyramid (O to L)\n",
    "norm=1\n",
    "#Type of normalization of 1 L2 and 2 Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors(gray,x):\n",
    "    if(x==1):\n",
    "        kpt,des=SIFTdetector.detectAndCompute(gray,None)\n",
    "    if(x==2):\n",
    "        kpt,des=SURFdetector.detectAndCompute(gray,None)\n",
    "    if(x==3):\n",
    "        kpt,des=ORBdetector.detectAndCompute(gray,None)\n",
    "    if(x==4):\n",
    "        step_size = 5\n",
    "        kpt = [cv2.KeyPoint(x, y, step_size) for y in range(0, gray.shape[0], step_size) \n",
    "                                            for x in range(0, gray.shape[1], step_size)]\n",
    "        kpt,des=SIFTdetector.compute(gray,kpt)\n",
    "    if(x==5):\n",
    "        step_size = 5\n",
    "        kpt = [cv2.KeyPoint(x, y, step_size) for y in range(0, gray.shape[0], step_size) \n",
    "                                            for x in range(0, gray.shape[1], step_size)]\n",
    "        kpt,des=SURFdetector.compute(gray,kpt)\n",
    "    return kpt, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(hist1):\n",
    "    if(norm==1):\n",
    "        hist2=np.linalg.norm(hist1, ord=2)\n",
    "        normhist=hist1 / hist2\n",
    "    if(norm==2):\n",
    "        hist=np.linalg.norm(hist1, ord=2)\n",
    "    return normhist       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BOW</h1>\n",
    "We compute the indicated descriptor for all the train images and subsequently build a numpy array with all the descriptors stacked together, then we   compute a k-means clustering on the descriptor space, where we test 3 different codebook sizes k (64,128,256). And, for each train image, we project each keypoint descriptor to its closest visual word. We represent each of the images with the frequency of each visual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bow(train_images_filenames,train_labels):\n",
    "    for filename,labels in zip(train_images_filenames,train_labels):\n",
    "        ima=cv2.imread(filename)\n",
    "        gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        kpt,des=descriptors(gray,x)\n",
    "        \n",
    "        Train_descriptors.append(des)\n",
    "        \n",
    "        Train_label_per_descriptor.append(labels)\n",
    "    D=np.vstack(Train_descriptors)\n",
    "    k = 256\n",
    "    codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "    codebook.fit(D)\n",
    "    \n",
    "    visual_words=np.zeros((len(Train_descriptors),k),dtype=np.float32)\n",
    "    for i in range(len(Train_descriptors)): \n",
    "        words=codebook.predict(Train_descriptors[i])\n",
    "        visual_words[i,:]=np.bincount(words,minlength=k)\n",
    "    \n",
    "    \n",
    "    return visual_words\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_descriptors = []\n",
    "Train_label_per_descriptor = []\n",
    "visual_words=[]\n",
    "visual_words=Bow(train_images_filenames,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bow-Spatial Pyramid</h1>\n",
    "We compute the SIFT descriptors for all the train images and subsequently build a numpy array with all the descriptors stacked together. Only working for the level 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_spatialpyramid(train_images_filenames,train_labels):\n",
    "    \n",
    "    for l in range(L+1):\n",
    "        print(range(L))\n",
    "        if(l==0):\n",
    "            for filename,labels in zip(train_images_filenames,train_labels):\n",
    "                ima=cv2.imread(filename)\n",
    "                gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "                kpt,des=descriptors(gray,x)\n",
    "                Train_descriptors.append(des)\n",
    "            D=np.vstack(Train_descriptors)\n",
    "            codebook.fit(D)\n",
    "            visualwords=np.zeros((len(Train_descriptors),k),dtype=np.float32)\n",
    "            for i in range(len(Train_descriptors)): \n",
    "                words=codebook.predict(Train_descriptors[i])\n",
    "                visualwords[i,:]=np.bincount(words,minlength=k)\n",
    "            visual_words=(visualwords)\n",
    "        else:\n",
    "            \n",
    "            Train_descriptors.clear()\n",
    "           \n",
    "            for filename,labels in zip(train_images_filenames,train_labels):\n",
    "                ima=cv2.imread(filename)\n",
    "                gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "                wstep=np.int(gray.shape[0]/(2**(l)))\n",
    "                hstep=np.int(gray.shape[1]/(2**(l)))\n",
    "           \n",
    "                for u in range(0,gray.shape[0],wstep):\n",
    "                    for f in range(0,gray.shape[1],hstep):  \n",
    "                        kpt,des=descriptors(gray[u:u+hstep, f:f+wstep],x)\n",
    "                        Train_descriptors.append(des)\n",
    "            visualwords=np.zeros((len(Train_descriptors),k),dtype=np.float32)    \n",
    "            for i in range(len(Train_descriptors)): \n",
    "                words=codebook.predict(Train_descriptors[i])\n",
    "                visualwords[i,:]=np.bincount(words,minlength=k)\n",
    "            weight = 2**(L-l)           \n",
    "            visual_words.append(weight*normalize(visualwords))      \n",
    "    return visual_words\n",
    "                        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Train_descriptors = []\n",
    "Train_label_per_descriptor = []\n",
    "visual_words=[]\n",
    "visualwords=[]\n",
    "hist=[]\n",
    "k = 256\n",
    "L=0\n",
    "codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "i=0\n",
    "visual_words=get_spatialpyramid(train_images_filenames,train_labels);\"\"\"\n",
    "        \n",
    "  \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the SVM we create the histogram intersection kernel (it is not available in the sklearn library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_intersection(M,N):\n",
    "    M_samples, M_features = M.shape\n",
    "    N_samples, N_features = N.shape\n",
    "    \n",
    "    K_int= np.zeros(shape=(M_samples,N_samples),dtype= np.float)\n",
    "    for i in range(M_samples):\n",
    "        for j in range(N_samples):\n",
    "            K_int[i,j]=np.minimum(M[i,:],N[j,:]).sum()\n",
    "    \n",
    "    return K_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SVM kernels</h1>\n",
    "In this step we add the SVM classifier to train our data. Before train our visual words, we preprocess the data standardizing the features by removing the mean and scaling to unit variance. In our implementation we can choose between different types of kernels:\n",
    "\n",
    "- Linear Kernel --> 'linear'\n",
    "- Rbf Kernel --> 'rbf'\n",
    "- Polynomial Kernel --> 'poly'\n",
    "- Sigmoid Kernel --> 'sigmoid'\n",
    "- Histogram Intersection Kernel --> 'histogram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the type of kernel (histogram by default):\n",
    "\n",
    "Kernel= 'histogram'\n",
    "\n",
    "K_cv=3\n",
    "folds = list(StratifiedKFold(n_splits=K_cv, shuffle=True, random_state=16).split(visual_words, train_labels))\n",
    "for j, (train_idx, test_idx) in enumerate(folds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "\n",
    "    visual_words_train = visual_words[train_idx]\n",
    "    train_labels=np.asarray(train_labels)\n",
    "    labels_train = train_labels[train_idx]\n",
    "    visual_words_valid = visual_words[test_idx]\n",
    "    labels_valid= train_labels[test_idx]\n",
    "\n",
    "    stdSlr_valid= StandardScaler().fit(visual_words_valid)\n",
    "    scaler_valid= stdSlr_valid.transform(visual_words_valid)\n",
    "    stdSlr_train= StandardScaler().fit(visual_words_train)\n",
    "    scaler_train= stdSlr_train.transform(visual_words_train)\n",
    "    stdSlr= StandardScaler().fit(visual_words)\n",
    "    scaler = stdSlr.transform(visual_words)\n",
    "\n",
    "    if(Kernel=='linear'):\n",
    "        clf= svm.SVC(kernel='linear',C=1,gamma=0.002).fit(scaler_train,labels_train)\n",
    "    if(Kernel=='rbf'):\n",
    "        clf= svm.SVC(kernel='rbf',C=1,gamma=0.002).fit(scaler_train,labels_train)\n",
    "    if(Kernel=='poly'):\n",
    "        clf= svm.SVC(kernel='poly',C=1,gamma=0.002).fit(scaler_train,labels_train)\n",
    "    if(Kernel=='sigmoid'):\n",
    "        clf= svm.SVC(kernel='sigmoid',C=1,gamma=0.002).fit(scaler_train,labels_train)\n",
    "    if(Kernel=='histogram'): \n",
    "        kernel=histogram_intersection(scaler_train,scaler_train)\n",
    "        clf= svm.SVC(kernel='precomputed',C=1).fit(kernel,labels_train)   \n",
    "        \n",
    "#     todo: add cv score (WIP)\n",
    "\n",
    "#     #Getting Training Score\n",
    "#     score = clf.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "#     print('Train loss:', score[0])\n",
    "#     print('Train accuracy:', score[1])\n",
    "#     #Getting Test Score\n",
    "#     score = gmodel.evaluate(X_holdout, Y_holdout, verbose=0)\n",
    "#     print('Test loss:', score[0])\n",
    "#     print('Test accuracy:', score[1])\n",
    "\n",
    "#     #Getting validation Score.\n",
    "#     pred_valid=gmodel.predict(X_holdout)\n",
    "#     y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "#     #Getting Train Scores\n",
    "#     temp_train=gmodel.predict(X_train)\n",
    "#     y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of K value in KNN classifier we perform a 10 fold cross validation with our training set to estimate the optimal K value. Also we represent the evolution of the accuracy for each k value to see the influence of the k value:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = list(filter(lambda x: x % 2 == 0, list(range(1,50))))\n",
    "cv_scores = []\n",
    "\n",
    "for kk in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=kk,n_jobs=-1,metric='braycurtis')\n",
    "    scores = cross_val_score(knn, visual_words, train_labels, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "optimal_k = neighbors[cv_scores.index(max(cv_scores))]\n",
    "\n",
    "plt.title('Accuracy as a function of K')\n",
    "plt.plot(neighbors, cv_scores)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.show()\n",
    "#python2.x\n",
    "#print(\"The optimal k value is %d\" % optimal_k)\n",
    "#python3.x\n",
    "print(\"The optimal k value is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a k-nn classifier and train it with the train descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_cv=3\n",
    "folds = list(StratifiedKFold(n_splits=K_cv, shuffle=True, random_state=16).split(visual_words, train_labels))\n",
    "for j, (train_idx, test_idx) in enumerate(folds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "\n",
    "    visual_words_train = visual_words[train_idx]\n",
    "    train_labels=np.asarray(train_labels)\n",
    "    labels_train = train_labels[train_idx]\n",
    "    visual_words_valid = visual_words[test_idx]\n",
    "    labels_valid= train_labels[test_idx]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=optimal_k,n_jobs=-1,metric='braycurtis')\n",
    "    knn.fit(visual_words_train, labels_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up computing the test descriptors and compute the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_test_spatialpyramid(test_images_filenames):\n",
    "    for l in range(L+1):\n",
    "        print(range(L))\n",
    "        if(l==0):\n",
    "            for i in range(len(test_images_filenames)):\n",
    "                filename=test_images_filenames[i]\n",
    "                ima=cv2.imread(filename)\n",
    "                gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "                kpt,des=descriptors(gray,x)\n",
    "                Train_descriptors.append(des)\n",
    "            D=np.vstack(Train_descriptors)\n",
    "            codebook.fit(D)\n",
    "            visualwords=np.zeros((len(Train_descriptors),k),dtype=np.float32)\n",
    "            for i in range(len(Train_descriptors)): \n",
    "                words=codebook.predict(Train_descriptors[i])\n",
    "                visualwords[i,:]=np.bincount(words,minlength=k)\n",
    "            visual_words.append(normalize(visualwords))\n",
    "        else:\n",
    "            \n",
    "            Train_descriptors.clear()\n",
    "           \n",
    "            for i in range(len(test_images_filenames)):\n",
    "                filename=test_images_filenames[i]\n",
    "                ima=cv2.imread(filename)\n",
    "                gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "                wstep=np.int(gray.shape[0]/(2**(l)))\n",
    "                hstep=np.int(gray.shape[1]/(2**(l)))\n",
    "           \n",
    "                for u in range(0,gray.shape[0],wstep):\n",
    "                    for f in range(0,gray.shape[1],hstep):  \n",
    "                        kpt,des=descriptors(gray[u:u+hstep, f:f+wstep],x)\n",
    "                        Train_descriptors.append(des)\n",
    "            visualwords=np.zeros((len(Train_descriptors),k),dtype=np.float32)    \n",
    "            for i in range(len(Train_descriptors)): \n",
    "                words=codebook.predict(Train_descriptors[i])\n",
    "                visualwords[i,:]=np.bincount(words,minlength=k)\n",
    "            weight = 2**(L-l)           \n",
    "            visual_words.append(weight*normalize(visualwords))      \n",
    "    return visual_words\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"visual_words=[]\n",
    "visual_words=get_test_spatialpyramid(test_images_filenames)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"visual_words_test=np.zeros((len(test_images_filenames),k),dtype=np.float32)\n",
    "for i in range(len(test_images_filenames)):\n",
    "    filename=test_images_filenames[i]\n",
    "    ima=cv2.imread(filename)\n",
    "    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    kpt,des=descriptors(gray,x)\n",
    "        \n",
    "    words=codebook.predict(des)\n",
    "    visual_words_test[i,:]=np.bincount(words,minlength=k)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we perform the confusion matrix to evaluate our classifier with respect the 8 image classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(test_labels))\n",
    "\n",
    "if(Kernel== 'histogram'):\n",
    "    predict= histogram_intersection(stdSlr_train.transform(visual_words_test),scaler_train)\n",
    "    predictions= clf.predict(predict)\n",
    "else:\n",
    "    predictions= clf.predict(stdSlr_train.transform(visual_words_test))\n",
    "\n",
    "conf_matrix= confusion_matrix(test_labels,predictions)\n",
    "plt.figure()\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "thresh = conf_matrix.max() / 2.\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j],horizontalalignment=\"center\",color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provisional: I'm trying to perform more evaluation metrics\n",
    "\n",
    "precision, recall, fscore, support= precision_recall_fscore_support(test_labels, predictions,average='macro')\n",
    "\n",
    "print (precision*100)\n",
    "print (recall*100)\n",
    "print (fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the accuracy for SVM and KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Kernel == 'histogram':\n",
    "    accuracy_svm = 100*clf.score(predict, test_labels)\n",
    "else:\n",
    "    accuracy_svm = 100*clf.score(stdSlr.transform(visual_words_test), test_labels)\n",
    "print(accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_knn = 100*knn.score(visual_words_test, test_labels)\n",
    "print(accuracy_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the next tables we summarise all the results that we have obtained changing the different parameters and using different types of feature descriptors:\n",
    "\n",
    "SIFT:\n",
    "\n",
    "| Distance | Kbatch=128,Initial parameters | Kbatch=128,Optimal K |K=64,Initial parameters |\n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean |  53.5315 | 55.8859 | 54.3990 |\n",
    "| Chebyshev |  43.2465 | 44.6096 | 46.3444 |\n",
    "| Manhattan |  50.1858 | 52.4163 | 53.4076 |\n",
    "|  Hamming  |  35.1920 | 37.5464 | 38.16604|\n",
    "| Canberra  |  50.1858 | 51.6728 | 49.0706 |\n",
    "|Braycurtis |  57.1250 | 57.6208 | 54.7707 |\n",
    "\n",
    "| Distance | Kbatch=64,Optimal K| Kbatch=256,Initial parameters | KBatch=256,Optimal K| \n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean | 54.0272 | 49.3184 | 49.5662 | \n",
    "| Chebyshev | 47.4597 | 42.1313 | 42.9987 |\n",
    "| Manhattan | 53.7794 | 35.3159 | 35.4399 |\n",
    "|  Hamming  | 42.5030 | 21.6852 | 23.1722 |\n",
    "| Canberra  | 49.3184 | 47.5836 | 48.5749 | \n",
    "|Braycurtis | 55.0185 | 60.5947 | 61.2143 |\n",
    "\n",
    "SURF:\n",
    "\n",
    "| Distance | Kbatch=128,Initial parameters | Kbatch=128,Optimal K |K=64,Initial parameters |\n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean |  54.7707 | 57.0012 | 58.8599 |\n",
    "| Chebyshev |  46.8401 | 49.5662 | 52.1685 |\n",
    "| Manhattan |  53.5315 | 53.5315 | 57.8686 |\n",
    "|  Hamming  |  35.9355 | 37.1747 | 38.7856 |\n",
    "| Canberra  |  55.7620 | 56.2577 | 56.7534 |\n",
    "|Braycurtis |  62.5774 | 63.8166 | 61.8339 |\n",
    "\n",
    "| Distance | Kbatch=64,Optimal K| Kbatch=256,Initial parameters | KBatch=256,Optimal K| \n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean | 58.6121 | 50.5576 | 50.5576 | \n",
    "| Chebyshev | 52.6641 | 47.4597 | 46.2205 |\n",
    "| Manhattan | 59.1078 | 42.9987 | 43.2465 |\n",
    "|  Hamming  | 40.6443 | 29.8636 | 31.1028 |\n",
    "| Canberra  | 56.5055 | 52.4163 | 52.4163 |\n",
    "|Braycurtis | 61.5861 | 63.5687 | 62.3296 |\n",
    "\n",
    "ORB:\n",
    "\n",
    "| Distance | Kbatch=128,Initial parameters | Kbatch=128,Optimal K |K=64,Initial parameters |\n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean |  32.8376 | 36.4312 | 31.9702 |\n",
    "| Chebyshev |  25.7744 | 27.7571 | 28.3767 |\n",
    "| Manhattan |  33.2094 | 38.5377 | 32.4659 |\n",
    "|  Hamming  |  26.0223 | 29.1201 | 24.4114 |\n",
    "| Canberra  |  27.8810 | 30.6071 | 27.8810 |\n",
    "|Braycurtis |  32.4659 | 32.4659 | 30.6071 |\n",
    "\n",
    "| Distance | Kbatch=64,Optimal K| Kbatch=256,Initial parameters | KBatch=256,Optimal K| \n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean | 35.1920 | 34.6964 | 36.0594 | \n",
    "| Chebyshev | 26.2701 | 24.0396 | 23.7918 |\n",
    "| Manhattan | 36.6790 | 27.8810 | 29.1201 |\n",
    "|  Hamming  | 28.2527 | 20.8178 | 19.3308 |\n",
    "| Canberra  | 30.8550 | 31.2267 | 33.3333 |\n",
    "|Braycurtis | 31.2267 | 31.9702 | 31.2267 |\n",
    "\n",
    "Dense SURF:\n",
    "\n",
    "| Distance | Kbatch=128,Initial parameters | Kbatch=128,Optimal K |K=64,Initial parameters |\n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean |  62.0817 | 63.9405 |61.8339|\n",
    "| Chebyshev |  54.0272 | 54.1511 |57.8686|\n",
    "| Manhattan |  64.9318 | 64.9318 |62.4535|\n",
    "|  Hamming  |  42.0074 | 49.8141 |39.9008|\n",
    "| Canberra  |  63.4448 | 63.9405 |63.8166|\n",
    "|Braycurtis |  64.9318 | 65.1796 |62.8252|\n",
    "\n",
    "| Distance | Kbatch=64,Optimal K| Kbatch=256,Initial parameters | KBatch=256,Optimal K| \n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean | 61.8339 | 64.6840 | 64.1883 | \n",
    "| Chebyshev | 57.8686 | 55.8859 | 52.5402 |\n",
    "| Manhattan | 63.9405 | 67.4101 | 66.2949 |\n",
    "|  Hamming  | 44.1140 | 50.8054 | 53.5315 |\n",
    "| Canberra  | 64.1883 | 62.2057 | 63.4448 |\n",
    "|Braycurtis | 63.5687 | 67.1623 | 66.4188 |\n",
    "\n",
    "Dense SIFT:\n",
    "\n",
    "| Distance | Kbatch=128,Initial parameters | Kbatch=128,Optimal K |K=64,Initial parameters |\n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean |  70.1363 | 72.7385 | 69.7645 |\n",
    "| Chebyshev |  58.7360 | 58.8599 | 58.9838 | \n",
    "| Manhattan |  76.7038 | 78.3147 | 73.3581 | \n",
    "|  Hamming  |  36.3073 | 38.6617 | 40.5204 |\n",
    "| Canberra  |  74.2255 | 75.4646 | 71.1276 |\n",
    "|Braycurtis |  76.7038 | 78.3147 | 73.6059 |\n",
    "\n",
    "| Distance | Kbatch=64,Optimal K| Kbatch=256,Initial parameters | KBatch=256,Optimal K| \n",
    "| --- | --- | --- | --- |\n",
    "| Euclidean | 73.1102  | 71.6232 | 71.3754 |\n",
    "| Chebyshev | 62.7013  | 54.8946 | 53.5315 |\n",
    "| Manhattan | 75.8364  | 79.9256 | 81.1648 |\n",
    "|  Hamming  | 37.7942  | 36.9268 | 37.0508 |\n",
    "| Canberra  | 74.1016  | 76.7038 | 78.0669 |\n",
    "|Braycurtis | 75.8364  | 79.5539 | 81.0408 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in our results, we have achieved to improve the initial acurracy of 53.53 almost 30% more. The best result is using the dense SIFT, a codebook size of 256 and the Manhattan (or Braycurtis 81.0408 %) distance where we obtain a 81.1648 % of accuracy.\n",
    "\n",
    "During this first lab we can conclude that:\n",
    "\n",
    "- Respect to the feature descriptors, we see a slightly improvement using SURF respect SIFT. Despite this, in the dense version we have the better results, specially, in dense SIFT. The ORB descriptor tends to get worse the results respect SIFT and SURF.\n",
    "\n",
    "- Respect to the codebook size, we can see that using a size of 256 we achieved to slightly improve the accuracy.  However, the computational cost increase a lot at same time that we increase the size.\n",
    "\n",
    "- Respect to the knn distance, from the 6 distances that we have tested, Braycurtis and Manhanttan distances provided us the best results. In the other hand, we have the Chebyshev and Hamming distance where we obtained the lowest accuracy.\n",
    "\n",
    "- Respect to the number of neighbors, we can see that choosing the optimal k value the accuracy can increase until 5% more than using the initial parameter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
